# TopN问题
题目描述：100GB url文件，使用1GB内存计算出现次数top100的url和出现的次数

url的长度大部分都限制在4K，即4096字节

## 分析&大致流程：
这种属于典型的大数据处理问题，不能一次性将所有数据加载到内存中，需要做数据的拆分。具体拆分可以通过Hash函数将100GB的url文件映射到可以加载到内存的文件大小，
这里限制的内存大小为1Gb，考虑到系统的占用，可以将100GB的url文件通过Hash之后取模的方式拆分为200个文件，然后将每个拆分之后的小文件加载到内存，再针对每个
小文件通过hash_map统计每个小文件中url出现的次数，然后通过堆排序，建立元素为100的小顶堆，求出每个小文件中url的top100，然后将200个文件中求出的top100做
归并排序求出100GB文件中top100的url。

## 解决方案：
1. 分而治之（hash映射）
- 顺序读取文件，对于每行url，取Hash(url)/200，将该值存放到200个小文件中，这样每个小文件的大小大概是0.5GB，但是由于url的分布及hash函数的均匀性，可能会
导致有文件的大小超过1GB，则继续按照Hash取模的方式（模的数字可以大致等于该文件大小/0.5G），将大文件拆分为小文件，直至所有小文件大小都不超过1GB。

2. hash_map统计/堆排序
- 对于每个小文件，加载到内存中通过hash_map的方式计算每个小文件中url出现的次数，在通过建立大小为100的小顶堆，筛选出出现top100的url

3. 归并排序
- 对于每个小文件中top100的url，通过归并排序，选出总体数据中的top100


## TODO
上面的思路方案只是简单的说明，具体的编码在后续同步实现，编码的具体实现就需要考虑的更多了，代码的规范，命名习惯，程序效率等等。。
